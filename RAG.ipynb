{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5d8e8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "# 1. ë°ì´í„° ë¡œë“œ\n",
    "with open(\"prompt_dataset_input_label22.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "# 2. ê°„ë‹¨í•œ ì „ì²˜ë¦¬ í•¨ìˆ˜\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"[^\\uAC00-\\uD7A3a-zA-Z0-9\\s.,!?]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def clean_raw_data(data, min_len=5, max_len=300):\n",
    "    seen = set()\n",
    "    cleaned = []\n",
    "    for item in data:\n",
    "        inp = clean_text(item[\"input\"])\n",
    "        tgt = clean_text(item[\"label\"])\n",
    "        if min_len <= len(inp) <= max_len and inp not in seen:\n",
    "            cleaned.append({\"input\": inp, \"label\": tgt})\n",
    "            seen.add(inp)\n",
    "    return cleaned\n",
    "\n",
    "# 3. ì „ì²˜ë¦¬ ì ìš©\n",
    "cleaned_data = clean_raw_data(raw_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf6bac44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lkj48\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "c:\\Users\\lkj48\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# 1. ì„ë² ë”© ëª¨ë¸ ë¡œë“œ\n",
    "embed_model = SentenceTransformer(\"jhgan/ko-sbert-sts\")  # ë˜ëŠ” klue/roberta ê³„ì—´\n",
    "\n",
    "# 2. good_promptë§Œ ì¶”ì¶œ\n",
    "good_prompts = [item[\"label\"] for item in cleaned_data]  # ë„ˆì˜ í•™ìŠµ json ê¸°ì¤€\n",
    "\n",
    "# 3. ë²¡í„°í™”\n",
    "vectors = embed_model.encode(good_prompts, convert_to_numpy=True)\n",
    "\n",
    "# 4. FAISS ì¸ë±ìŠ¤ ìƒì„±\n",
    "index = faiss.IndexFlatL2(vectors.shape[1])\n",
    "index.add(vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05c073ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# 1. tokenizerëŠ” ì›ë³¸ ëª¨ë¸ì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸° (ë³€ê²½ë˜ì§€ ì•ŠìŒ)\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"KETI-AIR/ke-t5-base\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 2. fine-tuned modelì€ ë„ˆê°€ ì§€ì •í•œ ì €ì¥ ë””ë ‰í† ë¦¬ì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"./t5_finetuned_result\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "493eaa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_similar_prompts(user_input, top_k=3):\n",
    "    query_vector = embed_model.encode([user_input])\n",
    "    _, indices = index.search(np.array(query_vector), top_k)\n",
    "    return [good_prompts[i] for i in indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6800beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_output(text):\n",
    "    text = text.replace(\"<pad>\", \"\").replace(\"</s>\", \"\")\n",
    "    text = re.sub(r\"\\b(\\w+)(\\s+\\1)+\\b\", r\"\\1\", text)  # ë°˜ë³µ ë‹¨ì–´ ì œê±°\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)  # ì¤‘ë³µ ê³µë°± ì œê±°\n",
    "    sentences = re.split(r'(?<=[.?!])\\s+|\\n+', text.strip())\n",
    "    return sentences[0].strip() if sentences else text.strip()\n",
    "\n",
    "\n",
    "def rag_refine_prompt(bad_prompt, max_new_tokens=64):\n",
    "    examples = retrieve_similar_prompts(bad_prompt)\n",
    "\n",
    "    # ì˜ˆì‹œ í¬í•¨í•œ í…œí”Œë¦¿\n",
    "    context = \"\\n\".join([f\"ì˜ˆì‹œ{i+1}: {ex}\" for i, ex in enumerate(examples)])\n",
    "    prompt = (\n",
    "        f\"ì‚¬ìš©ì ì…ë ¥: {bad_prompt}\\n\\n\"\n",
    "        f\"{context}\\n\\n\"\n",
    "        f\"ìœ„ì˜ ì˜ˆì‹œë¥¼ ì°¸ê³ í•´ì„œ í”„ë¡¬í”„íŠ¸ë¥¼ ë” êµ¬ì²´ì ì´ê³  ëª…í™•í•˜ê²Œ ë°”ê¿”ì£¼ì„¸ìš”.\\nì¶œë ¥:\"\n",
    "    )\n",
    "\n",
    "    # ìƒì„±\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(model.device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,\n",
    "        num_beams=4,\n",
    "        no_repeat_ngram_size=3,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return clean_output(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47ad6e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ ì…ë ¥: AIê°€ ë­ì•¼?\n",
      "âœ… ì¶œë ¥: ì¸ê³µì§€ëŠ¥ì´ ì‚¬íšŒ ì „ë°˜ì—ì„œ ì–´ë–»ê²Œ í™œìš©ë˜ëŠ”ì§€ ë¶„ì•¼ë³„ë¡œ ì˜ˆì‹œë¥¼ ë“¤ì–´ ì„¤ëª…í•´ì¤˜.\n",
      "============================================================\n",
      "ğŸ“ ì…ë ¥: í˜¼ì ìˆì„ë•Œ ë…¸ë˜ ë­ë“¤ì–´\n",
      "âœ… ì¶œë ¥: ê°€ì‚¬ ì „ë‹¬ë ¥ê³¼ ë¶„ìœ„ê¸°ê°€ ì¢‹ì€ ê°ì„±ê³¡ 3ê³¡ì„ ì¶”ì²œí•˜ê³  ê°€ì‚¬ ë°°ê²½ë„ í•¨ê»˜ ì•Œë ¤ì¤˜.\n",
      "============================================================\n",
      "ğŸ“ ì…ë ¥: ì„ ë¬¼ ì¶”ì²œí•´ì¤˜\n",
      "âœ… ì¶œë ¥: í™ˆì‡¼í•‘ì—ì„œ ë°°ì†¡ì´ ê°€ëŠ¥í•œ ì‡¼í•‘ëª° 3ê³³ê³¼ ì¶”ì²œ ì œí’ˆì„ ì†Œê°œí•´ì¤˜.\n",
      "============================================================\n",
      "ğŸ“ ì…ë ¥: ìŠ¤ë§ˆíŠ¸í° ì¶”ì²œ\n",
      "âœ… ì¶œë ¥: ìœ íŠœë¸Œì™€ ë°©ì†¡ í´ë¦½ì—ì„œ í™”ì œê°€ ëœ ì œí’ˆ ì¤‘ í‰ì ì´ ë†’ì€ ì œí’ˆ 3ê°€ì§€ë¥¼ ì¶”ì²œí•´ì¤˜.\n",
      "============================================================\n",
      "ğŸ“ ì…ë ¥: ì „ì‹œíšŒ ì¶”ì²œ\n",
      "âœ… ì¶œë ¥: ì „ì‹œê°€ ìì£¼ ì—´ë¦¬ëŠ” ì„œìš¸ ë‚´ ê°¤ëŸ¬ë¦¬ 3ê³³ì„ ì¶”ì²œí•˜ê³  ì „ì‹œ ì„±í–¥ê³¼ ê´€ëŒ ì •ë³´ë„ ì•Œë ¤ì¤˜.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "prompts = [\"AIê°€ ë­ì•¼?\", \"í˜¼ì ìˆì„ë•Œ ë…¸ë˜ ë­ë“¤ì–´\", \"ì„ ë¬¼ ì¶”ì²œí•´ì¤˜\", \"ìŠ¤ë§ˆíŠ¸í° ì¶”ì²œ\", \"ì „ì‹œíšŒ ì¶”ì²œ\"]\n",
    "\n",
    "for p in prompts:\n",
    "    result = rag_refine_prompt(p)\n",
    "    print(f\"ğŸ“ ì…ë ¥: {p}\")\n",
    "    print(f\"âœ… ì¶œë ¥: {result}\")\n",
    "    print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d22251d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.environ.get(\"JAVA_HOME\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5df48842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Java\\jdk-21\\bin\\server\\jvm.dll\n"
     ]
    }
   ],
   "source": [
    "import jpype\n",
    "print(jpype.getDefaultJVMPath())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ece362f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad BLEU ì ìˆ˜: 1.0\n",
      "Good BLEU ì ìˆ˜: 1.0\n",
      "í–¥ìƒëœ ì •ë„ (Good - Bad): 0.0\n"
     ]
    }
   ],
   "source": [
    "## bad_prompt , good_prompt í˜•íƒœì†Œ ë¹„êµ ##\n",
    "from konlpy.tag import Okt\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "def get_morphs(text):\n",
    "    return okt.morphs(text)\n",
    "\n",
    "def morph_bleu_self(text):\n",
    "    tokens = get_morphs(text)\n",
    "    return sentence_bleu([tokens], tokens, smoothing_function=SmoothingFunction().method1)\n",
    "\n",
    "# ì˜ˆì‹œ\n",
    "bad = \"AIê°€ ë­ì•¼?\"\n",
    "good = \"ì¸ê³µì§€ëŠ¥ì´ ì‚¬íšŒ ì „ë°˜ì—ì„œ ì–´ë–»ê²Œ í™œìš©ë˜ëŠ”ì§€ ë¶„ì•¼ë³„ë¡œ ì˜ˆì‹œë¥¼ ë“¤ì–´ ì„¤ëª…í•´ì¤˜.\"\n",
    "\n",
    "bad_score = morph_bleu_self(bad)\n",
    "good_score = morph_bleu_self(good)\n",
    "\n",
    "print(\"Bad BLEU ì ìˆ˜:\", round(bad_score, 4))\n",
    "print(\"Good BLEU ì ìˆ˜:\", round(good_score, 4))\n",
    "print(\"í–¥ìƒëœ ì •ë„ (Good - Bad):\", round(good_score - bad_score, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "183ec868",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lkj48\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\accelerate\\utils\\modeling.py:1390: UserWarning: Current model requires 14681976.0 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:01<00:00,  7.59it/s]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n"
     ]
    }
   ],
   "source": [
    "#### ì‹¤ì œ LLM ëª¨ë¸ì´ bad_prompt, good_prompt ë„£ì–´ì„œ ë‹µë³€ ë¹„êµí•´ë³´ê¸° ####\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# KoAlpaca ëª¨ë¸ëª…\n",
    "model_name = \"beomi/KoAlpaca-Polyglot-5.8B\"\n",
    "\n",
    "# í† í¬ë‚˜ì´ì € ë° ëª¨ë¸ ë¡œë“œ\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9f572d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"AIê°€ ë­ì•¼?\",\n",
    "    \"ì¸ê³µì§€ëŠ¥ì´ ì‚¬íšŒ ì „ë°˜ì—ì„œ ì–´ë–»ê²Œ í™œìš©ë˜ëŠ”ì§€ ë¶„ì•¼ë³„ë¡œ ì˜ˆì‹œë¥¼ ë“¤ì–´ ì„¤ëª…í•´ì¤˜.\"\n",
    "]\n",
    "\n",
    "for i, p in enumerate(prompts, 1):\n",
    "    input_text = f\"### ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸:\\n{p}\\n\\n### ì¶œë ¥:\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # âš ï¸ token_type_ids ì œê±°\n",
    "    if \"token_type_ids\" in inputs:\n",
    "        del inputs[\"token_type_ids\"]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=128,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            temperature=0.7,\n",
    "            repetition_penalty=1.2,\n",
    "            no_repeat_ngram_size=3,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    output = tokenizer.decode(output_ids[0], skip_special_tokens=True).replace(input_text, \"\").strip()\n",
    "    print(f\"ğŸ“ ì…ë ¥ {i}: {p}\")\n",
    "    print(f\"âœ… ìƒì„± ê²°ê³¼: {output}\")\n",
    "    print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a73a9628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª ì •ë³´ëŸ‰ ë¶„ì„ ê²°ê³¼ (Bigram ê¸°ì¤€)\n",
      "ğŸ“„ ìƒì„± ê²°ê³¼ 1 ì—”íŠ¸ë¡œí”¼: 3.9069\n",
      "ğŸ“„ ìƒì„± ê²°ê³¼ 2 ì—”íŠ¸ë¡œí”¼: 5.3923\n",
      "â¡ï¸ ì •ë³´ëŸ‰ ì¦ê°€: 1.4854\n"
     ]
    }
   ],
   "source": [
    "### ì •ë³´ëŸ‰ ë¶„ì„ ###\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# í˜•íƒœì†Œ ë¶„ì„ê¸°\n",
    "okt = Okt()\n",
    "\n",
    "def get_morphs(text):\n",
    "    return okt.morphs(text)\n",
    "\n",
    "def ngram_entropy(tokens, n=2):\n",
    "    ngrams = list(zip(*[tokens[i:] for i in range(n)]))\n",
    "    total = len(ngrams)\n",
    "    if total == 0:\n",
    "        return 0.0\n",
    "    counts = Counter(ngrams)\n",
    "    probs = [count / total for count in counts.values()]\n",
    "    entropy = -sum(p * np.log2(p) for p in probs)\n",
    "    return entropy\n",
    "\n",
    "# ì˜ˆì‹œ ìƒì„± ê²°ê³¼\n",
    "gen1 = \"ì¸ê³µì§€ëŠ¥(AI)ì€ ì¸ê°„ì´ í”„ë¡œê·¸ë˜ë°ì„ í†µí•´ ë§Œë“¤ì–´ë‚¸ ì§€ëŠ¥ì…ë‹ˆë‹¤.\"\n",
    "gen2 = (\"ì‚°ì—… ìë™í™” ë° ìë™íŒë§¤ê¸°, ë¬´ì¸ íƒë°°/ìš°í¸ë¬¼ ë°°ë‹¬, ìŒì„±ì¸ì‹ ê°€ì „ì œí’ˆ ì œì–´, ê°€ìƒ ë³´ì¡°ì¸/ì±—ë´‡, \"\n",
    "        \"ì˜ë£Œì§„ë‹¨ ë° ì¹˜ë£Œ ì§€ì›, ê¸ˆìœµì •ë³´ë¶„ì„ ë“±ì˜ ë¶„ì•¼ì—ì„œ ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì„ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# í˜•íƒœì†Œ ì¶”ì¶œ\n",
    "tokens1 = get_morphs(gen1)\n",
    "tokens2 = get_morphs(gen2)\n",
    "\n",
    "# ì •ë³´ëŸ‰ ê³„ì‚°\n",
    "entropy1 = ngram_entropy(tokens1)\n",
    "entropy2 = ngram_entropy(tokens2)\n",
    "\n",
    "# ì¶œë ¥\n",
    "print(\"ğŸ§ª ì •ë³´ëŸ‰ ë¶„ì„ ê²°ê³¼ (Bigram ê¸°ì¤€)\")\n",
    "print(f\"ğŸ“„ ìƒì„± ê²°ê³¼ 1 ì—”íŠ¸ë¡œí”¼: {round(entropy1, 4)}\")\n",
    "print(f\"ğŸ“„ ìƒì„± ê²°ê³¼ 2 ì—”íŠ¸ë¡œí”¼: {round(entropy2, 4)}\")\n",
    "print(f\"â¡ï¸ ì •ë³´ëŸ‰ ì¦ê°€: {round(entropy2 - entropy1, 4)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
